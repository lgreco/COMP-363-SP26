{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e923898b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GET 200 https://encyclopedia.chicagohistory.org/pages/700005.html?entryA (len=6705)\n",
      "\n",
      "Found 3 entry links on the index page.\n",
      "\n",
      "GET 200 https://encyclopedia.chicagohistory.org/pages/700011.html (len=5868)\n",
      "  !! Could not extract body from https://encyclopedia.chicagohistory.org/pages/700011.html\n",
      "GET 200 https://encyclopedia.chicagohistory.org/pages/700013.html (len=3749)\n",
      "  !! Could not extract body from https://encyclopedia.chicagohistory.org/pages/700013.html\n",
      "GET 200 https://encyclopedia.chicagohistory.org/pages/700031.html (len=7661)\n",
      "  !! Could not extract body from https://encyclopedia.chicagohistory.org/pages/700031.html\n",
      "\n",
      "Done. Wrote 0 entries. Skipped 3. Output: encyclopedia_chicago_entries_A.txt\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib3\n",
    "\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "BASE = \"https://encyclopedia.chicagohistory.org\"\n",
    "INDEX = BASE + \"/pages/700005.html?entryA\"\n",
    "\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120 Safari/537.36\"\n",
    "}\n",
    "\n",
    "def fetch(url: str) -> str:\n",
    "    r = requests.get(url, headers=HEADERS, timeout=30, verify=False, allow_redirects=True)\n",
    "    # Helpful debug\n",
    "    print(f\"GET {r.status_code} {r.url} (len={len(r.text)})\")\n",
    "    r.raise_for_status()\n",
    "    return r.text\n",
    "\n",
    "def collect_entry_links(index_html: str) -> list[str]:\n",
    "    soup = BeautifulSoup(index_html, \"html.parser\")\n",
    "    links = set()\n",
    "\n",
    "    for a in soup.find_all(\"a\", href=True):\n",
    "        href = a[\"href\"].strip()\n",
    "\n",
    "        # Typical entry links on that site look like: /pages/123.html\n",
    "        if re.match(r\"^/pages/\\d+\\.html$\", href):\n",
    "            links.add(BASE + href)\n",
    "\n",
    "    return sorted(links)\n",
    "\n",
    "def extract_entry_text(entry_html: str, url: str) -> str | None:\n",
    "    soup = BeautifulSoup(entry_html, \"html.parser\")\n",
    "\n",
    "    # Title: try a few common patterns\n",
    "    title = None\n",
    "    if soup.find(\"h1\"):\n",
    "        title = soup.find(\"h1\").get_text(\" \", strip=True)\n",
    "    if not title and soup.title:\n",
    "        title = soup.title.get_text(\" \", strip=True)\n",
    "\n",
    "    # Body: try common containers, then fall back to <article> or largest <div>\n",
    "    candidates = []\n",
    "    for selector in [\n",
    "        (\"div\", {\"id\": \"content\"}),\n",
    "        (\"div\", {\"id\": \"main\"}),\n",
    "        (\"div\", {\"class\": \"content\"}),\n",
    "        (\"div\", {\"class\": \"entry\"}),\n",
    "        (\"article\", None),\n",
    "    ]:\n",
    "        tag, attrs = selector\n",
    "        node = soup.find(tag, attrs) if attrs else soup.find(tag)\n",
    "        if node:\n",
    "            text = node.get_text(\"\\n\", strip=True)\n",
    "            if text and len(text) > 500:  # ignore tiny nav chunks\n",
    "                candidates.append(text)\n",
    "\n",
    "    if not candidates:\n",
    "        # Fallback: pick the largest div by text length\n",
    "        divs = soup.find_all(\"div\")\n",
    "        best = \"\"\n",
    "        for d in divs:\n",
    "            t = d.get_text(\"\\n\", strip=True)\n",
    "            if len(t) > len(best):\n",
    "                best = t\n",
    "        if len(best) > 800:\n",
    "            candidates.append(best)\n",
    "\n",
    "    if not candidates:\n",
    "        print(f\"  !! Could not extract body from {url}\")\n",
    "        return None\n",
    "\n",
    "    body = max(candidates, key=len)\n",
    "\n",
    "    # Light cleanup: remove repeated whitespace\n",
    "    body = re.sub(r\"\\n{3,}\", \"\\n\\n\", body).strip()\n",
    "\n",
    "    if not title:\n",
    "        title = url\n",
    "\n",
    "    return f\"{title}\\n{url}\\n\\n{body}\\n\"\n",
    "\n",
    "def main():\n",
    "    index_html = fetch(INDEX)\n",
    "    entry_urls = collect_entry_links(index_html)\n",
    "\n",
    "    print(f\"\\nFound {len(entry_urls)} entry links on the index page.\\n\")\n",
    "    if len(entry_urls) == 0:\n",
    "        print(\"No entry links found. The index page HTML structure may differ.\")\n",
    "        return\n",
    "\n",
    "    out_path = \"encyclopedia_chicago_entries_A.txt\"\n",
    "    written = 0\n",
    "    skipped = 0\n",
    "\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for i, url in enumerate(entry_urls, start=1):\n",
    "            try:\n",
    "                html = fetch(url)\n",
    "                entry = extract_entry_text(html, url)\n",
    "                if entry:\n",
    "                    f.write(\"=\" * 80 + \"\\n\")\n",
    "                    f.write(entry + \"\\n\\n\")\n",
    "                    written += 1\n",
    "                else:\n",
    "                    skipped += 1\n",
    "            except Exception as e:\n",
    "                print(f\"  !! Error on {url}: {e}\")\n",
    "                skipped += 1\n",
    "\n",
    "            if i % 10 == 0:\n",
    "                print(f\"Progress: {i}/{len(entry_urls)} (written={written}, skipped={skipped})\")\n",
    "\n",
    "            time.sleep(0.5)  # be polite\n",
    "\n",
    "    print(f\"\\nDone. Wrote {written} entries. Skipped {skipped}. Output: {out_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
